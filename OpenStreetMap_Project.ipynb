{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrangling Data from OpenStreetMap for the Greater Salt Lake City Area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "I have used Mapzen (https://mapzen.com/) to extract a dataset from OpenStreetMap. The area I have chosen is Greater Salt Lake City. The total uncompressed size of this file is 318 MB.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Processing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import pprint\n",
    "import xml.etree.ElementTree as ET  \n",
    "\n",
    "OSM_FILE = \"SaltLakeMap.osm\"  \n",
    "SAMPLE_FILE = \"sample.osm\"\n",
    "\n",
    "k = 25 # Take every 25th top level element\n",
    "\n",
    "def get_element(OSM_FILE, tags=('node', 'way', 'relation')):\n",
    "    \"\"\"Yield element if it is the right type of tag\n",
    "\n",
    "    Reference:\n",
    "    http://stackoverflow.com/questions/3095434/inserting-newlines-in-xml-file-generated-via-xml-etree-elementtree-in-python\n",
    "    \"\"\"\n",
    "    context = iter(ET.iterparse(OSM_FILE, events=('start', 'end')))\n",
    "    _, root = next(context)\n",
    "    for event, elem in context:\n",
    "        if event == 'end' and elem.tag in tags:\n",
    "            yield elem\n",
    "            root.clear()\n",
    "\n",
    "\n",
    "with open(SAMPLE_FILE, 'wb') as output:\n",
    "    output.write('<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n')\n",
    "    output.write('<osm>\\n  ')\n",
    "\n",
    "    \n",
    "    for i, element in enumerate(get_element(OSM_FILE)):\n",
    "        if i % k == 0:\n",
    "            output.write(ET.tostring(element, encoding='utf-8'))\n",
    "\n",
    "    output.write('</osm>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code takes a sample of the full OSM dataset for the Greater Salt Lake City area so we can explore a smaller, more managable amount of data. This will save time by allowing us to test our scripts before attempting to wrangle the entire 381 MB dataset. The sample.osm file we created with the above script is 6.44 MB. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auditing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'node': 61085, 'nd': 70493, 'member': 219, 'tag': 30395, 'relation': 42, 'way': 7302, 'osm': 1}\n"
     ]
    }
   ],
   "source": [
    "tags = {}\n",
    "\n",
    "def count_tags(SAMPLE_FILE):\n",
    "        \n",
    "        for event, elem in ET.iterparse(SAMPLE_FILE):\n",
    "            if elem.tag in tags.keys():\n",
    "                tags[elem.tag]+= 1\n",
    "            else:\n",
    "                tags[elem.tag] = 1\n",
    "                \n",
    "count_tags(SAMPLE_FILE)        \n",
    "print tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code tells us which data primitives there are in the SAMPLE_FIlE and gives us a sense of how many of each data primitive we can expect in the full file. An explanation of these data primitives can be found here: https://wiki.openstreetmap.org/wiki/OSM_XML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'problemchars': 0, 'lower': 15834, 'other': 3317, 'lower_colon': 11244}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import xml.etree.cElementTree as ET\n",
    "import pprint\n",
    "\n",
    "lower = re.compile(r'^([a-z]|_)*$') # for tags that contain only lowercase letters and are valid\n",
    "lower_colon = re.compile(r'^([a-z]|_)*:([a-z]|_)*$') # for otherwise valid tags with a colon in their names\n",
    "problemchars = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]') # for tags with problematic characters \n",
    "\n",
    "def key_type(element, keys):\n",
    "    if element.tag == \"tag\":\n",
    "        k = element.get(\"k\")\n",
    "        if re.search(lower,k):\n",
    "            keys['lower'] +=1\n",
    "        elif re.search(lower_colon,k):\n",
    "            keys['lower_colon'] +=1\n",
    "        elif re.search(problemchars,k):\n",
    "            keys['problemchars'] +=1\n",
    "        else:\n",
    "            keys['other'] +=1\n",
    "    \n",
    "    return keys\n",
    "\n",
    "def process_map(SAMPLE_FILE):\n",
    "    keys = {\"lower\": 0, \"lower_colon\": 0, \"problemchars\": 0, \"other\": 0}\n",
    "    for event, element in ET.iterparse(SAMPLE_FILE, events=(\"start\",)):\n",
    "        keys = key_type(element, keys)\n",
    "        \n",
    "    print keys\n",
    "        \n",
    "process_map(SAMPLE_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code loops through all of the tags named \"tag\" rather than only those tags named \"tag\" within the \"ways\" tag as the code below does. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'problemchars': 0, 'lower': 12677, 'other': 2647, 'lower_colon': 10469}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import xml.etree.cElementTree as ET\n",
    "import pprint\n",
    "from collections import defaultdict\n",
    "\n",
    "lower = re.compile(r'^([a-z]|_)*$') \n",
    "lower_colon = re.compile(r'^([a-z]|_)*:([a-z]|_)*$') \n",
    "problemchars = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')  \n",
    "\n",
    "def key_type(element, keys):\n",
    "    if element.tag == \"way\":\n",
    "        for tag in element.iter(\"tag\"):\n",
    "            k = tag.get(\"k\")\n",
    "            if lower.search(k):\n",
    "                keys['lower'] +=1\n",
    "            elif lower_colon.search(k):\n",
    "                keys['lower_colon'] +=1\n",
    "            elif problemchars.search(k):\n",
    "                keys['problemchars'] +=1\n",
    "            else:\n",
    "                keys['other'] +=1\n",
    "    \n",
    "    return keys\n",
    "              \n",
    "def process_map(SAMPLE_FILE):\n",
    "    keys = {\"lower\": 0, \"lower_colon\": 0, \"problemchars\": 0, \"other\": 0}\n",
    "    for event, element in ET.iterparse(SAMPLE_FILE, events=(\"start\",)):\n",
    "        keys = key_type(element, keys)\n",
    "        \n",
    "    print keys\n",
    "        \n",
    "process_map(SAMPLE_FILE)\n",
    "\n",
    "def test():\n",
    "    keys = process_map(SAMPLE_FILE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This piece of code takes SAMPLE_FILE and uses iterparse to find all of the top-level \"start\" tags titled \"way.\" Then within this \"way\" tags, the script finds the tags labeled \"tag\" and gets the k values within these tags to see if the values within those tags are address values and don't contain problem characters. The script then checks whether the k values include only lowercase letters, lowercase letters with colons, or a predefined set of problem characters. The script then adds the number of elements that meet each on of these constraints in a dictionary titled \"keys\" and outputs that dictionary.        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "405\n"
     ]
    }
   ],
   "source": [
    "def get_user(element):\n",
    "    if element.tag == \"way\":\n",
    "       user = element.get(\"uid\")\n",
    "\n",
    "       return user\n",
    "    \n",
    "def process_map(SAMPLE_FILE):\n",
    "    users = set()\n",
    "    for event, element in ET.iterparse(SAMPLE_FILE):\n",
    "        if get_user(element):\n",
    "           users.add(get_user(element))       \n",
    "       \n",
    "    print len(users)\n",
    "\n",
    "process_map(SAMPLE_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above counts the number of unique users who have contributed to the \"way\" tags for OpenStreetMap for Greater Salt Lake City. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the Data and Converting it to the CSV Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(set,\n",
       "            {'1300': {'1300'},\n",
       "             '319': {'S Sleepy Ridge Drive Suite 319'},\n",
       "             'A': {'S State St #A'},\n",
       "             'N': {'W 800 N'},\n",
       "             'Rd.': {'Portobello Rd.', 'West Portobello Rd.'}})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xml.etree.cElementTree as ET\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import pprint\n",
    "\n",
    "\n",
    "street_type_re = re.compile(r'\\b\\S+\\.?$', re.IGNORECASE) #pulls out last word when tag attribute is \"v\"\n",
    "\n",
    "\n",
    "expected = [\"Street\", \"Avenue\", \"Boulevard\", \"Drive\", \"Court\", \"Place\", \"Square\", \"Lane\", \"Road\", \n",
    "            \"Trail\", \"Parkway\", \"Commons\", \"Circle\", \"West\", \"East\", \"North\", \"South\", \"Temple\", \"Way\", \"Gateway\", \"Broadway\"]\n",
    "\n",
    "mapping = { \"Rd.\": \"Road\",\n",
    "            \"1300\": \"\",\n",
    "            \"N\": \"North\",\n",
    "            \"319\": \"\",\n",
    "            \"A\": \"\"\n",
    "            }\n",
    "def audit_street_type(street_types, street_name):\n",
    "    m = street_type_re.search(street_name)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        if street_type not in expected:\n",
    "            street_types[street_type].add(street_name)\n",
    "\n",
    "def is_street_name(elem):\n",
    "    return (elem.attrib['k'] == \"addr:street\")\n",
    "\n",
    "def audit(SAMPLE_FILE):\n",
    "    open(SAMPLE_FILE, \"r\")\n",
    "    street_types = defaultdict(set)\n",
    "    for event, elem in ET.iterparse(SAMPLE_FILE, events=(\"start\",)):\n",
    "        if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_street_name(tag):\n",
    "                    audit_street_type(street_types, tag.attrib['v'])\n",
    "    \n",
    "    return street_types\n",
    "\n",
    "audit(SAMPLE_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I first ran this auditing script, I found a lot of addresses that ended with words that were not in the \"expected\" list but which are perfectly acceptable for Salt Lake City. These include addresses ending with \"North,\"South,\" \"East,\" and \"West.\" I also found some ending with \"Circle\" (and others), which are perfectly acceptable. Thus, I have added these words to the expected list above so that they will be ignored. \n",
    "\n",
    "In my sample file, I am only left with one incorrect street nameâ€”one that ends in \"Rd.\" instead of \"Road.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1300': set(['1300']),\n",
      " '319': set(['S Sleepy Ridge Drive Suite 319']),\n",
      " 'A': set(['S State St #A']),\n",
      " 'N': set(['W 800 N']),\n",
      " 'Rd.': set(['Portobello Rd.', 'West Portobello Rd.'])}\n",
      "S State St #A => S State St #\n",
      "S Sleepy Ridge Drive Suite 319 => S Sleepy Ridge Drive Suite \n",
      "Portobello Rd. => Portobello Road\n",
      "West Portobello Rd. => West Portobello Road\n",
      "1300 => \n",
      "W 800 N => W 800 North\n"
     ]
    }
   ],
   "source": [
    "def update_name(name, mapping):\n",
    "\n",
    "    m = street_type_re.search(name)\n",
    "    if m.group() not in expected:\n",
    "        if m.group() in mapping.keys():\n",
    "            name = re.sub(m.group(), mapping[m.group()], name)\n",
    "    return name\n",
    "\n",
    "def test():\n",
    "    st_types = audit(SAMPLE_FILE)\n",
    "    pprint.pprint(dict(st_types))\n",
    "\n",
    "    for st_type, ways in st_types.iteritems(): \n",
    "        for name in ways:\n",
    "            better_name = update_name(name, mapping)\n",
    "            print name, \"=>\", better_name\n",
    "           \n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we can see how the street names will be cleaned when the are inserted into the CSV files (which comes below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import codecs\n",
    "import pprint\n",
    "import re\n",
    "import xml.etree.cElementTree as ET\n",
    "\n",
    "import cerberus\n",
    "\n",
    "import schema\n",
    "from audit import *  \n",
    "\n",
    "OSM_PATH = \"sample.osm\"\n",
    "\n",
    "NODES_PATH = \"nodes.csv\"\n",
    "NODE_TAGS_PATH = \"nodes_tags.csv\"\n",
    "WAYS_PATH = \"ways.csv\"\n",
    "WAY_NODES_PATH = \"ways_nodes.csv\"\n",
    "WAY_TAGS_PATH = \"ways_tags.csv\"\n",
    "\n",
    "LOWER_COLON = re.compile(r'^([a-z]|_)+:([a-z]|_)+')\n",
    "PROBLEMCHARS = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "SCHEMA = schema.schema\n",
    "\n",
    "\n",
    "NODE_FIELDS = ['id', 'lat', 'lon', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "NODE_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_FIELDS = ['id', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "WAY_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_NODES_FIELDS = ['id', 'node_id', 'position']\n",
    "\n",
    "\n",
    "def shape_element(element, node_attr_fields=NODE_FIELDS, way_attr_fields=WAY_FIELDS,\n",
    "                  problem_chars=PROBLEMCHARS, default_tag_type='regular'):\n",
    "    \"\"\"Clean and shape node or way XML element to Python dict\"\"\"\n",
    "\n",
    "    node_attribs = {}\n",
    "    way_attribs = {}\n",
    "    way_nodes = []\n",
    "    tags = []  \n",
    "    \n",
    "\n",
    "    if element.tag == 'node':\n",
    "        for attrib in element.attrib:\n",
    "            if attrib in NODE_FIELDS:\n",
    "                node_attribs[attrib] = element.attrib[attrib]\n",
    "                \n",
    "        for tag in element.iter(\"tag\"):\n",
    "            node_tags_attribs = {}\n",
    "            if LOWER_COLON.search(tag.attrib['k']):\n",
    "                node_tags_attribs['id'] = element.attrib['id']\n",
    "                node_tags_attribs['key'] = tag.attrib['k'].split(':',1)[1]\n",
    "                node_tags_attribs['value'] = tag.attrib['v']\n",
    "                node_tags_attribs['type'] = tag.attrib['k'].split(':',1)[0]\n",
    "                if tag.attrib['k'] == 'addr:street':\n",
    "                    node_tags_attribs['value'] = update_name(tag.attrib['v'], mapping)\n",
    "                else:\n",
    "                    node_tags_attribs['value'] = tag.attrib['v'] \n",
    "                tags.append(node_tags_attribs) \n",
    "              \n",
    "            elif PROBLEMCHARS.search(tag.attrib['k']):\n",
    "                continue \n",
    "                \n",
    "            else:\n",
    "                node_tags_attribs['id'] = element.attrib['id']\n",
    "                node_tags_attribs['key'] = tag.attrib['k']\n",
    "                node_tags_attribs['value'] = tag.attrib['v']\n",
    "                node_tags_attribs['type'] = 'regular'\n",
    "                tags.append(node_tags_attribs)\n",
    "                \n",
    "            \n",
    "        return {'node': node_attribs, 'node_tags': tags}\n",
    "    \n",
    "    elif element.tag == 'way':\n",
    "        for attrib in element.attrib:\n",
    "            if attrib in WAY_FIELDS:\n",
    "                way_attribs[attrib] = element.attrib[attrib]\n",
    "\n",
    "        position = 0        \n",
    "        for child in element:\n",
    "            way_node = {}\n",
    "            way_tag = {}\n",
    "\n",
    "            if  child.tag == 'tag':\n",
    "                if LOWER_COLON.match(child.attrib['k']):\n",
    "                    way_tag['id'] = element.attrib['id']\n",
    "                    way_tag['type'] = child.attrib['k'].split(':',1)[0]\n",
    "                    way_tag['key'] = child.attrib['k'].split(':',1)[1]\n",
    "                    way_tag['value'] = child.attrib['v']\n",
    "                    if child.attrib['k'] == 'addr:street':\n",
    "                        way_tag['value'] = update_name(child.attrib['v'], mapping)\n",
    "                    else:\n",
    "                        way_tag['value'] = child.attrib['v']\n",
    "                    tags.append(way_tag)\n",
    "                   \n",
    "                elif PROBLEMCHARS.match(child.attrib['k']):\n",
    "                       continue\n",
    "\n",
    "                else:\n",
    "                        way_tag['type'] = 'regular'\n",
    "                        way_tag['id'] = element.attrib['id']\n",
    "                        way_tag['key'] = child.attrib['k']\n",
    "                        way_tag['value'] = child.attrib['v']\n",
    "                        tags.append(way_tag)\n",
    "\n",
    "            elif  child.tag == 'nd':\n",
    "                way_node['id'] = element.attrib['id']\n",
    "                way_node['node_id'] = child.attrib['ref']\n",
    "                way_node['position'] = position\n",
    "                position += 1\n",
    "                way_nodes.append(way_node) \n",
    "    \n",
    "    return {'way': way_attribs, 'way_nodes': way_nodes, 'way_tags': tags}\n",
    "\n",
    "\n",
    "def get_element(SAMPLE_FILE, tags=('node', 'way', 'relation')):\n",
    "    \"\"\"Yield element if it is the right type of tag\"\"\"\n",
    "\n",
    "    context = ET.iterparse(SAMPLE_FILE, events=('start', 'end'))\n",
    "    _, root = next(context)\n",
    "    for event, elem in context:\n",
    "        if event == 'end' and elem.tag in tags:\n",
    "            yield elem\n",
    "            root.clear()\n",
    "\n",
    "\n",
    "def validate_element(element, validator, schema=SCHEMA):\n",
    "    \"\"\"Raise ValidationError if element does not match schema\"\"\"\n",
    "    if validator.validate(element, schema) is not True:\n",
    "        field, errors = next(validator.errors.iteritems())\n",
    "        message_string = \"\\nElement of type '{0}' has the following errors:\\n{1}\"\n",
    "        error_string = pprint.pformat(errors)\n",
    "        \n",
    "        raise Exception(message_string.format(field, error_string))\n",
    "\n",
    "\n",
    "class UnicodeDictWriter(csv.DictWriter, object):\n",
    "    \"\"\"Extend csv.DictWriter to handle Unicode input\"\"\"\n",
    "\n",
    "    def writerow(self, row):\n",
    "        super(UnicodeDictWriter, self).writerow({\n",
    "            k: (v.encode('utf-8') if isinstance(v, unicode) else v) for k, v in row.iteritems()\n",
    "        })\n",
    "\n",
    "    def writerows(self, rows):\n",
    "        for row in rows:\n",
    "            self.writerow(row)\n",
    "\n",
    "\n",
    "\n",
    "def process_map(file_in, validate):\n",
    "    \"\"\"Iteratively process each XML element and write to csv(s)\"\"\"\n",
    "\n",
    "    with codecs.open(NODES_PATH, 'w') as nodes_file, \\\n",
    "         codecs.open(NODE_TAGS_PATH, 'w') as nodes_tags_file, \\\n",
    "         codecs.open(WAYS_PATH, 'w') as ways_file, \\\n",
    "         codecs.open(WAY_NODES_PATH, 'w') as way_nodes_file, \\\n",
    "         codecs.open(WAY_TAGS_PATH, 'w') as way_tags_file:\n",
    "\n",
    "        nodes_writer = UnicodeDictWriter(nodes_file, NODE_FIELDS)\n",
    "        node_tags_writer = UnicodeDictWriter(nodes_tags_file, NODE_TAGS_FIELDS)\n",
    "        ways_writer = UnicodeDictWriter(ways_file, WAY_FIELDS)\n",
    "        way_nodes_writer = UnicodeDictWriter(way_nodes_file, WAY_NODES_FIELDS)\n",
    "        way_tags_writer = UnicodeDictWriter(way_tags_file, WAY_TAGS_FIELDS)\n",
    "\n",
    "        nodes_writer.writeheader()\n",
    "        node_tags_writer.writeheader()\n",
    "        ways_writer.writeheader()\n",
    "        way_nodes_writer.writeheader()\n",
    "        way_tags_writer.writeheader()\n",
    "\n",
    "        validator = cerberus.Validator()\n",
    "\n",
    "        for element in get_element(file_in, tags=('node', 'way')):\n",
    "            el = shape_element(element)\n",
    "            if el:\n",
    "                if validate is True:\n",
    "                    validate_element(el, validator)\n",
    "\n",
    "                if element.tag == 'node':\n",
    "                    nodes_writer.writerow(el['node'])\n",
    "                    node_tags_writer.writerows(el['node_tags'])\n",
    "                elif element.tag == 'way':\n",
    "                    ways_writer.writerow(el['way'])\n",
    "                    way_nodes_writer.writerows(el['way_nodes'])\n",
    "                    way_tags_writer.writerows(el['way_tags'])\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    process_map(OSM_PATH, validate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code takes all the data from the XML, cleans it, and puts it into CSV files that can then be added to an SQL database (which is done below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploring the Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import csv\n",
    "from pprint import pprint\n",
    "\n",
    "sqlite_file = 'SQL_Salt_Lake.db'\n",
    "\n",
    "conn = sqlite3.connect(sqlite_file)\n",
    "\n",
    "cur = conn.cursor()\n",
    "\n",
    "cur.execute('''DROP TABLE IF EXISTS nodes_tags''')\n",
    "conn.commit()\n",
    "\n",
    "cur.execute('''CREATE TABLE nodes_tags(id INTEGER, key TEXT, value TEXT, type TEXT, FOREIGN KEY(id) REFERENCES nodes(id))''');\n",
    "conn.commit()\n",
    "\n",
    "with open('nodes_tags.csv','rb') as fin:\n",
    "    dr = csv.DictReader(fin) \n",
    "    to_db = [(i['id'].decode(\"utf-8\"), i['key'].decode(\"utf-8\"), i['value'].decode(\"utf-8\"), i['type'].decode(\"utf-8\")) for i in dr]\n",
    "    \n",
    "\n",
    "cur.executemany(\"INSERT INTO nodes_tags(id, key, value,type) VALUES (?, ?, ?, ?);\", to_db)\n",
    "\n",
    "conn.commit()\n",
    "\n",
    "# cur.execute('SELECT * FROM nodes_tags')\n",
    "# all_rows = cur.fetchall()\n",
    "# print('1):')\n",
    "# pprint(all_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the above code, I have created a table called \"nodes_tags\" in the SQL database titled \"SQL_Salt_Lake\" and inserted the data from nodes_tags.csv into it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import csv\n",
    "from pprint import pprint\n",
    "\n",
    "sqlite_file = 'SQL_Salt_Lake.db'\n",
    "\n",
    "conn = sqlite3.connect(sqlite_file)\n",
    "\n",
    "cur = conn.cursor()\n",
    "\n",
    "cur.execute('''DROP TABLE IF EXISTS nodes''')\n",
    "conn.commit()\n",
    "\n",
    "cur.execute('''CREATE TABLE nodes(id INTEGER PRIMARY KEY NOT NULL, lat REAL, lon REAL, user TEXT, uid INTEGER, version INTEGER, changeset INTEGER, timestamp TEXT)''');\n",
    "conn.commit()\n",
    "\n",
    "with open('nodes.csv','rb') as fin:\n",
    "    dr = csv.DictReader(fin) \n",
    "    to_db = [(i['id'].decode(\"utf-8\"), i['lat'].decode(\"utf-8\"), i['lon'].decode(\"utf-8\"), i['user'].decode(\"utf-8\"), i['uid'].decode(\"utf-8\"), i['version'].decode(\"utf-8\"), i['changeset'].decode(\"utf-8\"), i['timestamp'].decode(\"utf-8\")) for i in dr]\n",
    "    \n",
    "\n",
    "cur.executemany(\"INSERT INTO nodes(id, lat, lon, user, uid, version, changeset, timestamp) VALUES (?, ?, ?, ?, ?, ?, ?, ?);\", to_db)\n",
    "\n",
    "conn.commit()\n",
    "\n",
    "# cur.execute('SELECT * FROM nodes')\n",
    "# all_rows = cur.fetchall()\n",
    "# print('1):')\n",
    "# pprint(all_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the above code, I have created a table called \"nodes\" in the SQL database titled \"SQL_Salt_Lake\" and inserted the data from nodes.csv into it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import csv\n",
    "from pprint import pprint\n",
    "\n",
    "sqlite_file = 'SQL_Salt_Lake.db'\n",
    "\n",
    "conn = sqlite3.connect(sqlite_file)\n",
    "\n",
    "cur = conn.cursor()\n",
    "\n",
    "cur.execute('''DROP TABLE IF EXISTS ways''')\n",
    "conn.commit()\n",
    "\n",
    "cur.execute('''CREATE TABLE ways(id INTEGER PRIMARY KEY NOT NULL, user TEXT, uid INTEGER, version TEXT, changeset INTEGER, timestamp TEXT)''');\n",
    "conn.commit()\n",
    "\n",
    "with open('ways.csv','rb') as fin:\n",
    "    dr = csv.DictReader(fin) \n",
    "    to_db = [(i['id'].decode(\"utf-8\"), i['user'].decode(\"utf-8\"), i['uid'].decode(\"utf-8\"), i['version'].decode(\"utf-8\"), i['changeset'].decode(\"utf-8\"), i['timestamp'].decode(\"utf-8\")) for i in dr]\n",
    "    \n",
    "\n",
    "cur.executemany(\"INSERT INTO ways(id, user, uid, version, changeset, timestamp) VALUES (?, ?, ?, ?, ?, ?);\", to_db)\n",
    "\n",
    "conn.commit()\n",
    "\n",
    "# cur.execute('SELECT * FROM ways')\n",
    "# all_rows = cur.fetchall()\n",
    "# print('1):')\n",
    "# pprint(all_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the above code, I have created a table called \"ways\" in the SQL database titled \"SQL_Salt_Lake\" and inserted the data from ways.csv into it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import csv\n",
    "from pprint import pprint\n",
    "\n",
    "sqlite_file = 'SQL_Salt_Lake.db'\n",
    "\n",
    "conn = sqlite3.connect(sqlite_file)\n",
    "\n",
    "cur = conn.cursor()\n",
    "\n",
    "cur.execute('''DROP TABLE IF EXISTS ways_tags''')\n",
    "conn.commit()\n",
    "\n",
    "cur.execute('''CREATE TABLE ways_tags(id INTEGER NOT NULL, key TEXT NOT NULL, value TEXT NOT NULL, type TEXT, FOREIGN KEY (id) REFERENCES ways(id))''');\n",
    "conn.commit()\n",
    "\n",
    "with open('ways_tags.csv','rb') as fin:\n",
    "    dr = csv.DictReader(fin) # comma is default delimiter\n",
    "    to_db = [(i['id'].decode(\"utf-8\"), i['key'].decode(\"utf-8\"), i['value'].decode(\"utf-8\"), i['type'].decode(\"utf-8\")) for i in dr]\n",
    "    \n",
    "\n",
    "cur.executemany(\"INSERT INTO ways_tags(id, key, value, type) VALUES (?, ?, ?, ?);\", to_db)\n",
    "\n",
    "conn.commit()\n",
    "\n",
    "# cur.execute('SELECT * FROM ways_tags')\n",
    "# all_rows = cur.fetchall()\n",
    "# print('1):')\n",
    "# pprint(all_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the above code, I have created a table called \"ways_tags\" in the SQL database titled \"SQL_Salt_Lake\" and inserted the data from ways_tags.csv into it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import csv\n",
    "from pprint import pprint\n",
    "\n",
    "sqlite_file = 'SQL_Salt_Lake.db'\n",
    "\n",
    "conn = sqlite3.connect(sqlite_file)\n",
    "\n",
    "cur = conn.cursor()\n",
    "\n",
    "cur.execute('''DROP TABLE IF EXISTS ways_nodes''')\n",
    "conn.commit()\n",
    "\n",
    "cur.execute('''CREATE TABLE ways_nodes(id INTEGER NOT NULL, node_id INTEGER NOT NULL, position INTEGER NOT NULL, FOREIGN KEY (id) REFERENCES ways(id), FOREIGN KEY (node_id) REFERENCES nodes (id))''');\n",
    "conn.commit()\n",
    "\n",
    "with open('ways_nodes.csv','rb') as fin:\n",
    "    dr = csv.DictReader(fin) \n",
    "    to_db = [(i['id'].decode(\"utf-8\"), i['node_id'].decode(\"utf-8\"), i['position'].decode(\"utf-8\")) for i in dr]\n",
    "    \n",
    "\n",
    "cur.executemany(\"INSERT INTO ways_nodes(id, node_id, position) VALUES (?, ?, ?);\", to_db)\n",
    "\n",
    "conn.commit()\n",
    "\n",
    "# cur.execute('SELECT * FROM ways_nodes');\n",
    "# all_rows = cur.fetchall()\n",
    "# print('1):')\n",
    "# pprint(all_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the above code, I have created a table called \"ways_nodes\" in the SQL database titled \"SQL_Salt_Lake\" and inserted the data from ways_nodes.csv into it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1):\n",
      "[(61085,)]\n"
     ]
    }
   ],
   "source": [
    "cur.execute('SELECT COUNT(*) FROM nodes');\n",
    "all_rows = cur.fetchall()\n",
    "print('1):')\n",
    "pprint(all_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I am counting the number of tags in the \"nodes\" table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1):\n",
      "[(3708,)]\n"
     ]
    }
   ],
   "source": [
    "cur.execute('SELECT COUNT(*) FROM nodes_tags');\n",
    "all_rows = cur.fetchall()\n",
    "print('1):')\n",
    "pprint(all_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I am counting the number of tags in the \"nodes_tags\" table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1):\n",
      "[(7302,)]\n"
     ]
    }
   ],
   "source": [
    "cur.execute('SELECT COUNT(*) FROM ways');\n",
    "all_rows = cur.fetchall()\n",
    "print('1):')\n",
    "pprint(all_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I am counting the number of tags in the \"ways\" table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1):\n",
      "[(26480,)]\n"
     ]
    }
   ],
   "source": [
    "cur.execute('SELECT COUNT(*) FROM ways_tags');\n",
    "all_rows = cur.fetchall()\n",
    "print('1):')\n",
    "pprint(all_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I am counting the number of tags in the \"ways_tags\" table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1):\n",
      "[(70493,)]\n"
     ]
    }
   ],
   "source": [
    "cur.execute('SELECT COUNT(*) FROM ways_nodes');\n",
    "all_rows = cur.fetchall()\n",
    "print('1):')\n",
    "pprint(all_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I am counting the number of tags in the \"ways_nodes\" table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1):\n",
      "[(794,)]\n"
     ]
    }
   ],
   "source": [
    "cur.execute('SELECT COUNT(DISTINCT(uid)) FROM (SELECT uid FROM nodes UNION ALL SELECT uid FROM ways)');\n",
    "all_rows = cur.fetchall()\n",
    "print('1):')\n",
    "pprint(all_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I am counting the number of unique users who contributed to the data in the OSM area I selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1):\n",
      "[(u'chadbunn', 9895),\n",
      " (u'osmjwh', 6360),\n",
      " (u'woodpeck_fixbot', 4288),\n",
      " (u'butlerm', 3495),\n",
      " (u'mash84121', 2866),\n",
      " (u'mvexel', 2747),\n",
      " (u'MelanieOriet', 2612),\n",
      " (u'Level', 2006),\n",
      " (u'carlotta4th', 1963),\n",
      " (u'OremSteve', 1735),\n",
      " (u'wrk3', 1614),\n",
      " (u'Ted Percival', 1387),\n",
      " (u'MasterOfKittens', 1305),\n",
      " (u'Val', 1259),\n",
      " (u'jackwiplock', 927),\n",
      " (u'TheDutchMan13', 903),\n",
      " (u'GaryOSM', 883),\n",
      " (u'nemmer', 799),\n",
      " (u'balcoath', 731),\n",
      " (u'bburgon42', 646)]\n"
     ]
    }
   ],
   "source": [
    "cur.execute('SELECT user, COUNT(*) as num FROM (SELECT user FROM nodes UNION ALL SELECT user FROM ways) GROUP BY user ORDER BY num DESC LIMIT 20');\n",
    "all_rows = cur.fetchall()\n",
    "print('1):')\n",
    "pprint(all_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I am counting the number of data entries each user contributed to the data in the OSM area I selected.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1):\n",
      "[(u'restaurant', 41),\n",
      " (u'place_of_worship', 32),\n",
      " (u'fast_food', 19),\n",
      " (u'fuel', 14),\n",
      " (u'parking', 14),\n",
      " (u'bank', 8),\n",
      " (u'bench', 8),\n",
      " (u'toilets', 8),\n",
      " (u'school', 6),\n",
      " (u'cafe', 5)]\n"
     ]
    }
   ],
   "source": [
    "cur.execute('SELECT value, COUNT(*) as num FROM nodes_tags WHERE key=\"amenity\" GROUP BY value ORDER BY num DESC LIMIT 10');\n",
    "all_rows = cur.fetchall()\n",
    "print('1):')\n",
    "pprint(all_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I am looking the number of each type of amenity found in the OSM area I selected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1):\n",
      "[(u'christian', 31)]\n"
     ]
    }
   ],
   "source": [
    "cur.execute('SELECT nodes_tags.value, COUNT(*) as num FROM nodes_tags WHERE nodes_tags.key=\"religion\" GROUP BY nodes_tags.value ORDER BY num DESC LIMIT 5');\n",
    "all_rows = cur.fetchall()\n",
    "print('1):')\n",
    "pprint(all_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I am looking at the number of places of worship by each religion type in the OSM area I selected.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1):\n",
      "[(u'mormon', 22),\n",
      " (u'latter_day_saints', 2),\n",
      " (u'catholic', 1),\n",
      " (u'jehovahs_witness', 1)]\n"
     ]
    }
   ],
   "source": [
    "cur.execute('SELECT nodes_tags.value, COUNT(*) as num FROM nodes_tags WHERE nodes_tags.key=\"denomination\" GROUP BY nodes_tags.value ORDER BY num DESC LIMIT 5');\n",
    "all_rows = cur.fetchall()\n",
    "print('1):')\n",
    "pprint(all_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I am looking at the denominations represented in the places of worship in the OSM area I selected.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1):\n",
      "[(u'The Church of Jesus Christ of Latter-day Saints', 22),\n",
      " (u'7-11', 3),\n",
      " (u'7-Eleven', 3),\n",
      " (u\"Arby's\", 3),\n",
      " (u'Burger King', 2)]\n"
     ]
    }
   ],
   "source": [
    "cur.execute('SELECT nodes_tags.value, COUNT(*) as num FROM nodes_tags WHERE nodes_tags.key=\"name\" GROUP BY nodes_tags.value ORDER BY num DESC LIMIT 5');\n",
    "all_rows = cur.fetchall()\n",
    "print('1):')\n",
    "pprint(all_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I was interested to see the names of the facilities of the places in my dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1):\n",
      "[(u'84105', 219),\n",
      " (u'84108', 127),\n",
      " (u'84106', 123),\n",
      " (u'84102', 56),\n",
      " (u'84116', 14),\n",
      " (u'84005', 11),\n",
      " (u'84096', 7),\n",
      " (u'84109', 7),\n",
      " (u'84084', 6),\n",
      " (u'84103', 5)]\n"
     ]
    }
   ],
   "source": [
    "cur.execute('SELECT tags.value, COUNT(*) as xyz FROM (SELECT * FROM nodes_tags UNION ALL SELECT * FROM ways_tags) tags WHERE tags.key=\"postcode\" GROUP BY tags.value ORDER BY xyz DESC LIMIT 10');\n",
    "all_rows = cur.fetchall()\n",
    "print('1):')\n",
    "pprint(all_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here, I wanted to see where most of the places in my OSM file were located. It looks like they are mainly concentrated around four zip codes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "I chose to look at every 25th data primitive in the OSM XML for Greater Salt Lake City because the full dataset crashed my computer when I tried to analyze it. This full file was .33 GB. The sample file was .013 GB. \n",
    "\n",
    "Here are a few interesting things I found about the data:\n",
    "\n",
    "- 794 unique users contributed data to the sample of the OSM XML I looked at\n",
    "- A single user, \"chadbunn\" contributed 20% of entries of the top 20 contibutors (9895/48420). \n",
    "- The most common amenity is a \"restaurant\"(41), which is followed by \"places of worship\" (32).\n",
    "- The only religion found at these places of worship is Christianity.\n",
    "- The denominations of these Christians are Mormon (24), Catholic (1), and Jehovas Witness (1). This seems reasonable considering Salt Lake City is the geographic center of the Mormon faith.\n",
    "- In the area under investigation, most of the names of the places are \"The Church of Jesus Christ of Latter-day Saints,\" and the rest are convienience stores and fast-food chains.\n",
    "- There are no zip codes in the sample that seem to be erroneous; they all appear to be in areas within the OSM XML area that was extracted. \n",
    "\n",
    "\n",
    "An area for further investigation would be to run an analysis on the entire data set and see how the religious makeup of the geographic area captured by the data in the OSM XLM aligns up with official statistics for religious affiliation for Salt Lake City and for Utah as a whole.  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
